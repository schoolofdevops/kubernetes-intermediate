---
phase: 03-core-content
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - docs/section-2/01-overview.mdx
  - docs/section-2/02-reading.md
  - docs/section-2/03-lab.md
  - docs/section-2/quiz.md
autonomous: true

must_haves:
  truths:
    - "Learner can read Module 2 content in 10-20 minutes and understand HPA, VPA, and KEDA concepts"
    - "Learner can follow the lab to configure HPA for vote/result services, generate load, and observe autoscaling"
    - "Learner can explain why HPA and VPA should not target the same Deployment for the same metric"
    - "Learner can answer 12-15 quiz questions about autoscaling strategies"
    - "Module 2 continues the 0-1-2 build-up sequence using the existing cluster and Voting App from Module 0"
  artifacts:
    - path: "docs/section-2/02-reading.md"
      provides: "Autoscaling concepts covering HPA, VPA, KEDA"
      min_lines: 180
    - path: "docs/section-2/03-lab.md"
      provides: "Autoscaling lab with Metrics Server, HPA, load testing, KEDA"
      min_lines: 280
    - path: "docs/section-2/quiz.md"
      provides: "12-15 autoscaling quiz questions"
      min_lines: 120
  key_links:
    - from: "docs/section-2/03-lab.md"
      to: "examples/voting-app/"
      via: "References Voting App from Module 0 as base"
      pattern: "vote"
    - from: "docs/section-2/03-lab.md"
      to: "docs/section-2/02-reading.md"
      via: "References HPA and KEDA concepts from reading"
      pattern: "HPA|KEDA"
---

<objective>
Create complete Module 2: Autoscaling - teaching learners to configure horizontal pod autoscaling (HPA) with Metrics Server, understand VPA recommendations, and explore event-driven scaling with KEDA. The Voting App evolves from static replicas to dynamic scaling (vote/result auto-scale on CPU, worker scales on Redis queue depth via KEDA).

Purpose: Autoscaling is the second production-readiness improvement. Learners see the Voting App respond dynamically to load, which demonstrates why static replicas are insufficient. This completes the 0-1-2 build-up sequence.

Output: Complete Module 2 (overview, reading, lab, quiz) with 3 Mermaid diagrams and a comprehensive lab covering Metrics Server setup, HPA configuration, load testing, and KEDA introduction.
</objective>

<execution_context>
@/Users/gshah/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gshah/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-core-content/03-CONTEXT.md
@.planning/phases/03-core-content/03-RESEARCH.md
@.planning/phases/03-core-content/03-01-SUMMARY.md
@templates/content-template.md
@templates/lab-template.md
@templates/quiz-template.md
@templates/AUTHORING-GUIDE.md
@docs/section-2/01-overview.mdx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Module 2 reading materials with overview update and Mermaid diagrams</name>
  <files>
    docs/section-2/01-overview.mdx
    docs/section-2/02-reading.md
  </files>
  <action>
**1. Update docs/section-2/01-overview.mdx:**
Replace placeholder. Create proper module landing page:
- Title: "Module 2: Autoscaling"
- Frontmatter: sidebar_position: 1, title: "Overview"
- Metadata: Difficulty: Intermediate, Estimated Time: 100 minutes (15 min reading + 60 min lab + 15 min quiz)
- "What You Will Learn" (5 items): Metrics Server installation and verification, HPA configuration with CPU targets, load testing to trigger autoscaling, VPA in recommendation mode, event-driven scaling with KEDA
- "Prerequisites": Modules 0 and 1 completed, KIND cluster running with scheduled Voting App
- Overview paragraphs: "Your Voting App has smart scheduling from Module 1, but every service still runs exactly one replica. What happens during a flash sale when thousands of votes pour in? Static replicas can't handle traffic spikes."

**2. Create docs/section-2/02-reading.md:**
Follow templates/content-template.md. Target: 12-18 minute read (~2500-3500 words). Conversational tone, no emojis.

Content structure:
- **Overview** (2-3 paragraphs): The scaling problem - static replicas under dynamic load. Why manual scaling (kubectl scale) is reactive, not proactive. Autoscaling makes the app self-adjusting.
- **Horizontal Pod Autoscaling (HPA)** section:
  - What HPA does: watches metrics, adjusts replicas automatically
  - The scaling loop: check metrics -> compute desired replicas -> scale -> wait -> repeat
  - Include Mermaid sequence diagram: HPA scaling decision loop (HPA -> Metrics API -> calculate desired -> API Server -> Deployment adjusts replicas, from research pattern)
  - autoscaling/v2 API: supports CPU, memory, custom metrics
  - Key config: minReplicas, maxReplicas, target utilization, behavior (stabilization windows, scale-up/down policies)
  - Include YAML example from research (hpa-vote.yaml with behavior section)
  - Explain stabilization window: "Scale up fast, scale down slow - prevents flapping"
  - Tip admonition: "Always set resource requests on pods using HPA. Without requests, HPA cannot calculate utilization percentage."
- **Metrics Server: The Missing Piece** section:
  - What Metrics Server provides (CPU/memory metrics via Metrics API)
  - Why it's not installed by default in KIND
  - How HPA uses it: usage / request = utilization percentage
  - Caution admonition: "HPA absolutely requires Metrics Server. No Metrics Server = no scaling." (from research Pitfall 1)
- **Vertical Pod Autoscaler (VPA)** section:
  - What VPA does: adjusts resource requests/limits instead of replica count
  - Three modes: Off (recommendations only), Initial (set on creation), Auto (live adjustment)
  - CRITICAL warning with danger admonition: "Never run HPA and VPA on the same Deployment for the same resource. VPA changes requests, which changes HPA's utilization calculation, causing thrashing." (from research Pitfall 4)
  - Include Mermaid diagram: HPA vs VPA comparison (side-by-side concept showing HPA adjusts replicas, VPA adjusts resources)
  - Practical pattern: Use VPA in Off mode to get recommendations, then apply manually and use HPA for scaling
- **Event-Driven Scaling with KEDA** section:
  - What KEDA adds: scale based on external events (queue length, HTTP requests, cron schedules)
  - Perfect for the worker: scale workers based on Redis queue depth (not CPU)
  - Include Mermaid diagram: KEDA workflow (Event Source -> KEDA Operator -> creates/manages HPA -> scales Deployment)
  - ScaledObject concept: connects event source to Deployment
  - Brief comparison: HPA = resource metrics, KEDA = event metrics
  - Info admonition: "KEDA is a CNCF graduated project with 65+ scalers. It complements HPA rather than replacing it."
- **Choosing the Right Scaling Strategy** section:
  - Table or decision guide: Use HPA for CPU/memory-based scaling (stateless services like vote/result). Use KEDA for event-driven scaling (queue consumers like worker). Use VPA recommendations to right-size before applying HPA.
- **Summary** with 5 bullet points
- **Further Reading**: K8s HPA docs, KEDA docs, Metrics Server docs
- Info admonition: "Next Steps - Time to make your Voting App auto-scale in the lab"
  </action>
  <verify>
    - Both files exist and are non-empty
    - `npm run build` succeeds
    - 02-reading.md contains at least 3 Mermaid diagram code blocks
    - 02-reading.md word count is 2500-3500 words
    - Content includes HPA/VPA conflict warning (danger admonition)
    - Content explains Metrics Server dependency
    - No emojis, all code blocks have language tags
  </verify>
  <done>Module 2 reading materials explain HPA, VPA, KEDA with 3 Mermaid diagrams, clear HPA/VPA conflict warning, and practical scaling strategy guidance targeting 12-18 min read time.</done>
</task>

<task type="auto">
  <name>Task 2: Create Module 2 lab and quiz</name>
  <files>
    docs/section-2/03-lab.md
    docs/section-2/quiz.md
  </files>
  <action>
**1. Create docs/section-2/03-lab.md:**
Follow templates/lab-template.md 8-section structure. Duration: 50-60 minutes. Builds on the Module 0 Voting App deployment (the 0-1-2 build-up sequence).

Title: "Lab: Making the Voting App Auto-Scale"

- **Objectives** (5 items): Install and verify Metrics Server on KIND, configure HPA for vote and result services, generate load and observe autoscaling in action, understand VPA recommendations, set up KEDA for event-driven worker scaling
- **Prerequisites**: Modules 0 and 1 completed, KIND cluster with 3 workers running, Voting App deployed with scheduling rules from Module 1, Resource requests must be set on vote/result Deployments (Task 1 will handle this)
- **Setup**: Verify cluster and app running. NOTE: vote/result Deployments from Module 0 don't have resource requests. First setup step is adding resource requests (cpu: 100m, memory: 128Mi requests; cpu: 200m, memory: 256Mi limits) to vote and result Deployments. This is required for HPA to work.
- **Tasks** (5 tasks + 1 challenge):
  - **Task 1: Install Metrics Server**:
    Apply Metrics Server YAML from official releases. Patch for KIND with `--kubelet-insecure-tls` flag (from research).
    Wait for Metrics Server to be ready. Verify with `kubectl top nodes` and `kubectl top pods`.
    Explain: "KIND uses self-signed certificates. The patch tells Metrics Server to skip TLS verification for kubelet connections."
    Include troubleshooting inline: if `kubectl top` returns error, wait 60s for metrics collection.
  - **Task 2: Add Resource Requests to Voting App**:
    Update vote and result Deployments to add resource requests/limits.
    Show the YAML patch or updated Deployment spec.
    Verify with `kubectl describe deployment vote` showing resource requests.
    Explain: "HPA calculates utilization as (current usage / request). Without requests, there's no denominator."
  - **Task 3: Configure HPA for Vote Service** (from research Example 4):
    Create HPA targeting vote Deployment: min 2, max 8, target CPU 50%.
    Include behavior section: scaleUp stabilizationWindowSeconds: 0 (immediate), scaleDown: 300s (wait 5 min).
    Apply and verify with `kubectl get hpa`.
    Watch HPA: `kubectl get hpa vote-hpa --watch`.
    Show initial state: TARGETS showing current/target percentage.
  - **Task 4: Generate Load and Observe Scaling** (from research Example 5):
    Deploy load generator pod using busybox hitting vote service.
    Watch HPA scale up in separate terminal (or use --watch).
    Show expected progression: replicas increase as CPU rises above target.
    Wait and verify: `kubectl get pods -l app=vote` shows multiple replicas.
    Stop load generator. Watch scale down (explain 5-min stabilization window).
    Explain: "The stabilization window prevents flapping - rapid scale up/down cycles."
    Functional verification: port-forward and confirm vote service still works under load.
  - **Task 5: KEDA for Event-Driven Worker Scaling**:
    Install KEDA (kubectl apply from official releases).
    Wait for KEDA operator to be ready.
    Create ScaledObject targeting worker Deployment, using Redis list scaler.
    Configure: trigger on Redis list length > 5, scale worker 1-5 replicas.
    Demonstrate: submit many votes rapidly, watch worker scale up as Redis queue grows.
    Verify: `kubectl get scaledobject`, `kubectl get hpa` (KEDA creates HPA automatically).
    Explain: "KEDA scales the worker based on actual work in the queue, not just CPU usage. This is more efficient for queue consumers."
  - **Challenge: HPA/VPA Conflict Demonstration**:
    Briefly install VPA CRDs (if time allows) or simulate the concept.
    Explain the conflict: if VPA changes requests from 100m to 50m CPU, HPA's 80m usage goes from 80% to 160% utilization instantly, triggering unnecessary scale-up.
    Key learning: "Never run HPA and VPA together on the same metric. Use VPA in recommendation mode, then apply requests manually."
    (This can be a conceptual explanation with kubectl describe showing recommendation rather than a full VPA install if time is tight.)
- **Verification**:
  1. `kubectl top pods` returns metrics for all pods
  2. `kubectl get hpa` shows vote-hpa with valid targets (not <unknown>)
  3. Vote service scales up under load (at least 3+ replicas observed)
  4. Vote service scales down after load removed (wait 5 min)
  5. KEDA ScaledObject exists and manages worker scaling
  6. Voting App still fully functional end-to-end
- **Cleanup**: Module 2 completes the 0-1-2 build-up sequence per the hybrid evolution approach. Modules 3 and 4 start with focused, clean deployments rather than carrying forward autoscaling state. Clean up autoscaling resources but keep the base cluster:
  `kubectl delete hpa vote-hpa`, `kubectl delete scaledobject worker-scaledobject`, delete load generator pod.
  Keep KIND cluster and base Voting App running. Learners will redeploy fresh from examples/ as needed in subsequent modules.
- **Troubleshooting**:
  1. HPA shows <unknown> targets (Metrics Server not ready or no resource requests - from research Pitfall 1)
  2. Pods don't scale up under load (check target utilization, verify requests are set, check `kubectl describe hpa` events)
  3. KEDA ScaledObject not working (KEDA operator not running, Redis connection issue)
- **Key Takeaways**: 5 points

Include all kubectl commands with expected output. Emphasize watching scaling in real-time.

**2. Create docs/section-2/quiz.md:**
Follow templates/quiz-template.md. Create 13 questions (8 MCQ, 3 scenario, 2 true/false).

Topics:
- HPA target utilization calculation (usage / request)
- Metrics Server purpose and why required
- Stabilization window purpose (prevent flapping)
- minReplicas vs maxReplicas behavior
- HPA/VPA conflict (scenario: what happens when both target CPU?)
- KEDA vs HPA (when to use which)
- Resource requests vs limits for autoscaling
- Scale-down delay purpose
- autoscaling/v2 vs v1 differences
- Load testing impact on HPA (scenario: predict replica count)
- VPA modes (Off, Initial, Auto)
- Event-driven scaling for queue consumers (scenario: best strategy for worker)
- Metrics API availability debugging

Module metadata: Module: 2, Topic: Autoscaling, Question Count: 13
  </action>
  <verify>
    - `docs/section-2/03-lab.md` exists with all 8 sections
    - Lab has 5 tasks + 1 challenge
    - Lab includes Metrics Server installation with KIND patch
    - Lab includes KEDA installation and ScaledObject creation
    - Lab includes load generation and observation steps
    - Lab Cleanup removes autoscaling resources but keeps cluster
    - `docs/section-2/quiz.md` has at least 13 questions
    - `npm run build` succeeds
    - No emojis, all code blocks have language tags
  </verify>
  <done>Module 2 lab walks learners through Metrics Server setup, HPA configuration, load testing, and KEDA event-driven scaling with the Voting App. Quiz has 13 questions testing autoscaling concepts including the HPA/VPA conflict scenario.</done>
</task>

</tasks>

<verification>
- Module 2 has 4 content files: 01-overview.mdx (updated), 02-reading.md, 03-lab.md, quiz.md
- Reading is 12-18 min with 3 Mermaid diagrams (HPA loop, HPA vs VPA, KEDA workflow)
- Lab follows 8-section template with 5 tasks + 1 challenge
- Lab builds on Module 0 Voting App (part of the 0-1-2 build-up sequence)
- Lab installs Metrics Server with KIND patch
- Lab includes real load testing and scaling observation
- Lab includes KEDA for event-driven scaling
- Quiz has 13 questions including HPA/VPA conflict scenario
- `npm run build` succeeds
</verification>

<success_criteria>
Module 2 is complete: autoscaling concepts explained with diagrams (12-18 min read), hands-on lab configuring HPA and KEDA on Voting App with real load testing (50-60 min), and quiz testing understanding (13 questions). This completes the Module 0-1-2 build-up sequence.
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-content/03-03-SUMMARY.md`
</output>
